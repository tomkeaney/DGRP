---
title: "Running GWAS on the DGRP phenotypes"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, results='hide'
)
```


```{r}
library(tidyverse)
library(bigsnpr) # to install:   devtools::install_github("privefl/bigsnpr")
library(glue)
library(dbplyr)
library(DBI)
library(RSQLite)
library(kableExtra)

# Database of variant and gene annotations
db <- DBI::dbConnect(RSQLite::SQLite(),
                     "~/Rprojects/fitnessGWAS/data/derived/annotations.sqlite3")

plink <- file.path(getwd(), "code/plink") # I used plink v1.9 for MacOS, available here: https://www.cog-genomics.org/plink/
options(readr.show_col_types = FALSE)


# Define a helper function to pass commands to the terminal
# Note that we set `intern = TRUE`, and pass the result of `system()` to `cat()`,
# ensuring that the Terminal output will be printed in this knitr report.
run_command <- function(shell_command, wd = getwd(), path = ""){
  cat(system(glue("cd ", wd, path, "\n", shell_command), intern = TRUE), sep = '\n')
}

save_manhattan_plot <- function(gwas_results, save_path){
  
  manhattan_data <- gwas_results %>%
    mutate(position = str_split(SNP, "_"),
           chr = map_chr(position, ~ .x[1]),
           position = as.numeric(map_chr(position, ~ .x[2])),
           pval = -1 * log10(P)) %>% 
    select(chr, position, pval)
  
  max_pos <- manhattan_data %>%
    group_by(chr) %>%
    summarise(
      max_pos = max(position), 
      middle_pos = (max_pos - min(position)) / 2,
      .groups = "drop") %>%
    as.data.frame()
  max_pos$max_pos <- c(0, cumsum(max_pos$max_pos[1:5]))
  max_pos$label_pos <- max_pos$max_pos + max_pos$middle_pos
  
  manhattan_data <- manhattan_data %>%
    left_join(max_pos, by = "chr") %>%
    mutate(position = position + max_pos)
  y_max <- max(manhattan_data$pval) * 1.08
  
  plot <- manhattan_data %>%
    ggplot(aes(position, pval, group = chr, fill = chr, stroke = 0.05)) +
    # geom_hline(yintercept = 5, linetype = 2, colour = "grey20") +
    geom_point(size = 0.9, colour="grey20", pch = 21) +
    scale_fill_brewer(palette = "Paired", name = "Chromosome") +
    scale_x_continuous(breaks = max_pos$label_pos, labels = max_pos$chr) +
    scale_y_continuous(expand = c(0,0), limits = c(0, y_max)) +
    ylab(expression(paste("-", log[10], " p value"))) + 
    xlab("Chromosome and position") +
    theme_bw() +
    theme(legend.position = "none",
          panel.grid.major.x = element_blank(), 
          panel.grid.minor.x = element_blank(),
          panel.border = element_blank(),
          axis.ticks.x = element_blank()) 
  
  ggsave(filename = save_path, plot = plot, height = 6, width = 6)
  rm(plot)
}

```


## Load trait data to be used in GWAS

Here we load the phenotype data to be used for the GWAS analyses below. This step depends upon the file `data/derived/all.dgrp.phenos_scaled.csv`, and so updating that file with newly-received data will cause the GWAS to be re-run. 

The other dependencies for this script are the files `gwas_data/input/dgrp2.[XX]` where `XX` is `bed`, `bim` and `fam` (obtained from the [Mackay lab website](http://dgrp2.gnets.ncsu.edu/))), or (if you'd like to skip several hours of imputing missing genotypes using Beagle) the files `gwas_data/derived/dgrp2_QC_all_lines_imputed_correct.[XX]`.

```{r}
meta_data <- read_csv("data/derived/meta_data_for_all_traits.csv")

# Get a list of phenotypic traits that were measured in at least 80 DGRP lines
traits_for_gwas <- meta_data %>% 
  filter(`# lines measured` >= 80) %>% 
  pull(Trait)

# Get the line mean phenotypes for all those traits
line_mean_phenotypes <- read_csv("data/derived/all.dgrp.phenos_scaled.csv") %>% 
  select(line, Trait, trait_value) %>% 
  filter(Trait %in% traits_for_gwas) %>% 
  mutate(line = paste("line", line, sep = ""))
```



## Perform SNP quality control and imputation

Before running GWAS analyses, we first cleaned up the DGRP's .bed/.bim/.fam files (available from the [Mackay lab website](http://dgrp2.gnets.ncsu.edu/)) as follows:

1. Remove any SNPs for which genotypes are missing for >10% of the DGRP lines. We then use the software [Beagle](https://faculty.washington.edu/browning/beagle/beagle.html) to impute the remaining missing genotypes.
2. Remove SNPs with a minor allele frequency of less than 5%.

Note that in the PLINK-formatted genotype files, lines fixed for the major allele are coded as 2, and lines fixed for the minor allele as 0. This means that in the association tests we calculate below, _negative_ effect sizes mean that the minor allele is associated with lower trait values, while _positive_ effect sizes means that the minor allele is associated with higher trait values. 

```{r QC_and_imputation, results='hide'}
perform_SNP_QC_and_imputation <- function(phenotypes){
  
  beagle <- bigsnpr::download_beagle()
  
  # Use Plink to clean and subset the DGRP's SNP data as follows:
  # Only keep SNPs for which at least 90% of DGRP lines were successfully genotyped (--geno 0.1)
  # Only keep SNPs with a minor allele frequency of 0.05 or higher (--maf 0.05)
  # Finally, write the processed BIM/BED/FAM files to the data/derived directory
  run_command(glue("{plink} --bfile dgrp2",
                   " --geno 0.1 --maf 0.05 --allow-no-sex", 
                   " --make-bed --out ../derived/dgrp2_QC_all_lines"), path = "/gwas_data/input/")
  
  # Use the shell command 'sed' to remove underscores from the DGRP line names in the .fam file (e.g. 'line_120' becomes 'line120')
  # Otherwise, these underscores cause trouble when we need to convert from PLINK to vcf format (vcf format uses underscore as a separator)
  for(i in 1:2) run_command("sed -i '' 's/_//' dgrp2_QC_all_lines.fam", path = "/gwas_data/derived/")
  
  # Now impute the missing genotypes using Beagle
  # This part uses the data for the full DGRP panel of >200 lines, to infer missing genotypes as accurately as possible. 
  # This step uses a lot of memory (I set to 28MB max, and it used 26.5GB), but maybe it can also run on a less powerful computer?
  # The bigsnpr package provides a helpful wrapper for Beagle called snp_beagleImpute(): it translates to a VCF file and back again using PLINK
  snp_beagleImpute(beagle, plink, 
                   bedfile.in = "gwas_data/derived/dgrp2_QC_all_lines.bed", 
                   bedfile.out = "gwas_data/derived/dgrp2_QC_all_lines_imputed.bed",
                   ncores = 7, 
                   memory.max = 20)
  
  # assign a sex of 'female' to all the DGRP lines (Beagle removes the sex, and it seems PLINK needs individuals to have a sex)
  run_command("sed -i '' 's/	0	0	0/	0	0	2/' dgrp2_QC_all_lines_imputed.fam", path = "/gwas_data/derived/")
  
  # Re-write the .bed file, to make sure the MAF and genotyping thresholds are correctly assigned post-Beagle
  run_command(glue("{plink} --bfile dgrp2_QC_all_lines_imputed",
                   " --geno 0.1 --maf 0.05", 
                   " --make-bed --out dgrp2_QC_all_lines_imputed_correct"), path = "/gwas_data/derived/")

  # Use PLINK to get the allele IDs and calculate the MAFs across the whole DGRP, for all SNPs that survived QC
  # The file created is called data/derived/plink.frq
  run_command("{plink} --bfile dgrp2_QC_all_lines_imputed_correct --freq", path = "/gwas_data/derived")

  # Clean up:
  unlink(c("gwas_data/derived/plink.log",
           "gwas_data/derived/dgrp2_QC_all_lines_imputed.bed",
           "gwas_data/derived/dgrp2_QC_all_lines_imputed.bim",
           "gwas_data/derived/dgrp2_QC_all_lines_imputed.fam",
           "gwas_data/derived/dgrp2_QC_all_lines_imputed.log",
           "gwas_data/derived/dgrp2_QC_all_lines_imputed_correct.log"))
}

# If the imputation is not already done, create the following 3 files of imputed genotype data: 
# dgrp2_QC_all_lines_imputed_correct.bed/bim/fam
if(!file.exists("gwas_data/derived/dgrp2_QC_all_lines_imputed_correct.bed")) {
  perform_SNP_QC_and_imputation(phenotypes = predicted_line_means)
}


# These are 205 genotyped DGRP lines. Double-check for incorrect line names in the phenotype data, and remove any
genotyped_lines <- read.table("gwas_data/derived/dgrp2_QC_all_lines_imputed_correct.fam")[,1]
line_mean_phenotypes <- line_mean_phenotypes %>% 
  filter(line %in% genotyped_lines)
```


## Create a reduced list of LD-pruned SNPs with PLINK

To keep the computation time and memory usage manageable, we do not save the effect sizes for every variant (SNPs and indels) that passed the MAF and missingness quality control step above (i.e. 1,646,652 variants), but rather we save the effect sizes for a subset of variants that were approximately in linkage disequilibrium. We identified this LD-pruned set of variants using the PLINK arguments `--indep-pairwise 100 10 0.2`, i.e. pruning within 100kB sliding windows, sliding 10 variants along with each step, and allowing a maximum pairwise $r^2$ threshold of 0.2 between loci. With these parameters, 1,420,071 variants were removed, leaving 226,581 for downstream analysis. Subsequent inspection of the Manhattan plots suggests that this method removes most (but not all) variants that are in complete or high LD.

```{r}
# indep-pairwise arguments are: 
# 100kB window size, 
# variant count to shift the window by 10 variants at the end of each step, 
# pairwise r^2 threshold of 0.2
run_command(glue("{plink} --bfile dgrp2_QC_all_lines_imputed_correct",
                 " --indep-pairwise 100 10 0.2"), path = "/gwas_data/derived/")

unlink("gwas_data/derived/plink.prune.out")
```



## Run all the GWAS
Note: because PLINK defines the minor allele as the alt allele (so, lines fixed for the minor allele are scored as genotype: 2, and those with the major allele as genotype: 0), a _positive_ effect size in these association tests means the _minor_ allele is associated with a _higher_ value of the trait in question.

```{r}
gwas_one_trait <- function(focal_phenotype){
  
  # First make 'focal_data', a 2-column data frame with the line and the focal phenotype value
  focal_data <- line_mean_phenotypes %>% 
    filter(Trait == focal_phenotype) %>% 
    spread(Trait, trait_value) 
    
  names(focal_data)[2] <- "focal_pheno"
  
  # Double check there are no missing values (shouldn't be any)
  focal_data <- focal_data %>% 
    filter(!is.na(line) & !is.na(focal_pheno))
  
  # Make a list of the lines in our sample and save as a text file for passing to PLINK
  lines_to_keep <- gsub("_", "", focal_data$line) %>% cbind(.,.)
  write.table(lines_to_keep, 
              row.names = FALSE, 
              col.names = FALSE, 
              file = "gwas_data/derived/lines_to_keep.txt", 
              quote = FALSE)
  
  # Now cull the PLINK files to just the lines that we measured, and re-apply the 
  # MAF cut-off of 0.05 for the new smaller sample of DGRP lines
  # note that this uses all the variants, not just LD-pruned set (via the -bfile argument)
  run_command(glue("{plink} --bfile dgrp2_QC_all_lines_imputed_correct",  
                   " --keep-allele-order", 
                   " --keep lines_to_keep.txt --geno 0.1 --maf 0.05", 
                   " --make-bed --out dgrp2_QC_focal_lines"), 
              path = "/gwas_data/derived/")
  
  # Define a function to add our phenotype data to a .fam file, which is 
  # needed for GWAS analysis and to make sure PLINK includes these samples
  # The 'phenotypes' data frame needs to have a column called 'line'
  add_phenotypes_to_fam <- function(filepath, focal_data){
    read_delim(filepath, col_names = FALSE, delim = " ") %>% 
      select(X1, X2, X3, X4, X5) %>% # Get all the non-phenotype columns
      left_join(focal_data, 
                by = c("X1" = "line")) %>%
      write.table(file = "gwas_data/derived/dgrp2_QC_focal_lines_NEW.fam", 
                  col.names = FALSE, row.names = FALSE, 
                  quote = FALSE, sep = " ")
    unlink("gwas_data/derived/dgrp2_QC_focal_lines.fam")
    file.rename("gwas_data/derived/dgrp2_QC_focal_lines_NEW.fam", 
                "gwas_data/derived/dgrp2_QC_focal_lines.fam")
  }
  
  add_phenotypes_to_fam("gwas_data/derived/dgrp2_QC_focal_lines.fam", focal_data)
 
  # # Write a file with the line and phenotype data called phenotype.txt, for gcta64
  # pheno_data <- focal_data %>% 
  #   mutate(line_copy = line) %>% 
  #   select(line, line_copy, focal_pheno) %>% as.matrix() 
  # pheno_data %>% 
  #   write.table(row.names = FALSE, col.names = FALSE, 
  #               file = "gwas_data/derived/phenotype.txt", quote = FALSE)

  # Run mixed-model GWAS (in practice, the relatedness between almost all pairs of lines 
  # is sufficiently low that PLINK always instead chooses to run a linear model)
  print(focal_phenotype)
  run_command("{plink} --bfile dgrp2_QC_focal_lines  --assoc --maf 0.05 --out gwas_results/new", 
              path = "/gwas_data/derived")
  
  gwas_results <- read.table("gwas_data/derived/gwas_results/new.qassoc", 
                             header = TRUE) %>% 
    select(SNP, BETA, SE, P)
  
  # Save a Manhattan plot for this trait, using the all variants (not just the LD subset):
  save_manhattan_plot(gwas_results,
                      glue("gwas_data/derived/manhattan_plots/{focal_phenotype}.jpeg"))
  
  # If the trait has a "/" in the name it cannot work as a file name. Change the "/" to "_"
  # focal_phenotype <- str_replace_all(focal_phenotype, "[/]", "_")

  # Save a file containing all SNPs with P < 1e-5:
  gwas_results %>% 
    filter(P < 1e-05) %>% 
    write_tsv(glue("gwas_data/derived/gwas_results/{focal_phenotype}_significant_SNPs.tsv.gz"))

  # Rename and compress the GWAS summary stats file 
  # The filter step means that only variants in the LD-pruned subset get saved to disk.
  gwas_results %>% 
    filter(SNP %in% (pull(read_tsv("gwas_data/derived/plink.prune.in", col_names = "SNP"), SNP))) %>% 
    write_tsv(glue("gwas_data/derived/gwas_results/{focal_phenotype}.tsv.gz"))
  unlink("gwas_data/derived/gwas_results/new.qassoc")
  
  # Rename the plink log file
  file.rename("gwas_data/derived/gwas_results/new.log",
              glue("gwas_data/derived/gwas_results/{focal_phenotype}_log.txt"))
}

# List the files in the gwas_results directory where association test results get stored:
files <- list.files("gwas_data/derived/gwas_results", 
                    pattern = "tsv.gz", 
                    full.names = TRUE)

# Make a list of the phenotypic traits that have already been done:
already_run_traits <- str_split(files, "/") %>% 
  map_chr(~ .x[4]) %>% 
  str_remove_all(".tsv.gz")

# List the phenotypic traits that have not yet been GWAS'ed
traits_to_run_for_gwas <- traits_for_gwas[
  !(traits_for_gwas %in% already_run_traits)]

# run all the remaining gwas, one-by-one
lapply(traits_to_run_for_gwas, gwas_one_trait)

# Delete files no longer needed
unlink(list.files("gwas_data/derived", 
                  pattern = "focal_lines", full.names = T))
unlink(c("gwas_data/derived/lines_to_keep.txt"))
unlink(c("gwas_data/derived/gwas_results/new.qassoc", "gwas_data/derived/gwas_results/new.log"))
```

## Details about the significant variants

### Create a table of information about all the significant variants

The code below creates a large file (>12MB) containing all the significant variants (defined as $P < 10^{-5}$) for all the traits, alongside some annotations for each variant (e.g. the gene associated with the variant, the site class of the variant, and its alleles and minor allele frequencies) plus the summary statistics from the GWAS (`BETA` is the effect size of the association, where _negative_ effect sizes mean that the minor allele is associated with lower trait values, while _positive_ effect sizes means that the minor allele is associated with higher trait values, SE is the standard error, and P is the p-value). This information is saved in the file `gwas_data/derived/all_traits_significant_SNPs.tsv`, which is accessible on the Github repository.

```{r}
if(length(traits_to_run_for_gwas) > 0){
  
  top_hits_files <- list.files("gwas_data/derived/gwas_results", full.names = TRUE, pattern = "significant_SNPs")
  
  add_annotation <- function(filepath){
    focal <- read_tsv(filepath) %>% 
      arrange(P)
    snp_order <- focal$SNP
    trait <- str_remove_all(filepath, "gwas_data[/]derived[/]gwas_results[/]")
    trait <- str_remove_all(trait, "_significant_SNPs[.]tsv[.]gz")
    print(trait)
    tbl(db, "variants") %>% 
      filter(SNP %in% !!focal$SNP) %>% 
      select(-chr, -position) %>% 
      collect() %>% 
      left_join(focal, by = "SNP") %>% 
      mutate(log10_P = -1 * log10(P),
             SNP = factor(SNP, snp_order)) %>% 
      arrange(SNP) %>% 
      rename_all(~ str_replace_all(.x, "[.]", "_")) %>% 
      mutate(trait = trait, .before = 1)
    
  }
  
  map_df(top_hits_files, add_annotation)  %>% 
    left_join(tbl(db, "genes") %>% select(FBID, gene_name) %>% collect(), by = "FBID") %>% 
    select(trait, SNP, FBID, gene_name, everything()) %>% 
    mutate(gene_name = replace(gene_name, is.na(FBID), NA)) %>% 
    write_tsv("gwas_data/derived/all_traits_significant_SNPs.tsv")
}
```

### Various summary statistics about the significant variants

#### Version of the table that is correct with the originally-published dataset:

The table shows various counts about the significant variants, as well as the mean and standard error MAF (minor allele frequency) for all the significant variants as well as the total set of variants that was used in the GWAS (i.e. all those with minor allele frequency >5% and missingness < 10%). Given that the choice of significance threshold is arbitrary, we use two different definitions of statistical significance: $P < 10^{-5}$ (which is used in many studies of the DGRP) and $P < 10^{-8}$ (which is commonly used in human GWAS).

```{r results='show'}
all_sig_snps <- read_tsv("gwas_data/derived/all_traits_significant_SNPs.tsv") %>% 
  left_join(meta_data, by = c("trait" = "Trait"))

make_full_tally_table <- function(all_sig_snps){
  
  # Count number of GWAS that were run
  gwas_results_files <- list.files("gwas_data/derived/gwas_results/")
  gwas_results_files <- gwas_results_files[!str_detect(gwas_results_files, "_significant_SNPs")]
  gwas_results_files <- gwas_results_files[!str_detect(gwas_results_files, "log[.]txt")]
  num_gwas_run <- length(gwas_results_files)
  
  # Count number of unique studies that provided data for GWAS
  num_unique_studies_used_for_gwas <- meta_data %>% 
    filter(Trait %in% str_remove_all(gwas_results_files, "[.]tsv[.]gz")) %>% 
    distinct(Reference) %>% nrow()
  
  # A function to randomly select one trait per study, from among the traits measured in at least 80 DGRP lines
  get_independent_traits <- function(){
    set.seed(1) # Set the seed for reproducibility
    read_csv("data/derived/meta_data_for_all_traits.csv") %>%
      filter(`# lines measured` >= 80) %>% # Only analyse traits that measured at least 80 lines
      group_by(Reference) %>%
      sample_n(1) %>% # Randomly sample one phenotype per study
      pull(Trait) %>% sort() 
  }
  
  mean_and_SE <- function(x){
    mean_x <- format(round(mean(x), 2), nsmall = 2)
    se_x <- format(round(sd(x) / sqrt(length(x)), 3), nsmall = 3)
    if(se_x == "0.000") se_x <- "<0.001"
    paste(mean_x, " (", se_x, ")", sep = "")
  }
  
  
  maf_all_snps_tests <- tbl(db, "variants") %>% 
    filter(MAF > 0.05) %>% 
    pull(MAF) %>% mean_and_SE()
  
  
  make_tally_table <- function(all_sig_snps){
    
    prettify <- function(x) format(x, big.mark=",")
    
    num_traits_affected <- all_sig_snps %>% 
      distinct(SNP, trait) %>% 
      group_by(SNP) %>% 
      summarise(n = n()) 
    
    num_traits_affected_indep <- all_sig_snps %>% 
      filter(trait %in% get_independent_traits()) %>% 
      distinct(SNP, trait) %>% 
      group_by(SNP) %>% 
      summarise(n = n()) 
    
    num_snps_sig_in_2_studies <- sum(pull(num_traits_affected, n) >= 2)
    num_snps_sig_in_3_studies <- sum(pull(num_traits_affected, n) >= 3)
    num_snps_sig_in_2_studies_indep <- sum(pull(num_traits_affected_indep, n) >= 2)
    num_snps_sig_in_3_studies_indep <- sum(pull(num_traits_affected_indep, n) >= 3)
    
    num_snps_sig_in_2_studies <- paste(prettify(num_snps_sig_in_2_studies), " (", num_snps_sig_in_2_studies_indep, ")", sep = "")
    num_snps_sig_in_3_studies <- paste(prettify(num_snps_sig_in_3_studies), " (", num_snps_sig_in_3_studies_indep, ")", sep = "")
    
    tibble(
      `Number of studies that provided data for GWA tests` = num_unique_studies_used_for_gwas,
      `Number of traits analysed using GWA tests` = num_gwas_run,
      `Number of studies with at least one significant SNP` = all_sig_snps %>% distinct(Reference) %>% nrow(),
      `Number of phenotypic traits with at least one significant SNP` = all_sig_snps %>% distinct(trait) %>% nrow(),
      `Number of variants significantly associated with at least one phenotypic trait` = all_sig_snps %>% distinct(SNP) %>% nrow() %>% prettify(),
      `Number of variants significantly associated with at least two phenotypic traits (which were measured in different studies)` = num_snps_sig_in_2_studies,
      `Number of variants significantly associated with at least three phenotypic traits (which were measured in different studies)` = num_snps_sig_in_3_studies,
      `Number of genes close to one of the significant variants` = all_sig_snps %>% filter(!is.na(FBID)) %>% distinct(FBID) %>% nrow() %>% prettify(),
      `MAF of the significant variants (mean and SE)` = all_sig_snps %>% distinct(SNP, MAF) %>% pull(MAF) %>% mean_and_SE(),
      `MAF of all variants included in the GWAS (mean and SE)` = maf_all_snps_tests) %>% 
      gather(Quantity, n)
  }
  
  left_join(
    all_sig_snps %>% 
      filter(log10_P >= 5) %>% 
      make_tally_table() %>% 
      rename(`Using p < 10-5` = n),
    
    all_sig_snps %>% 
      filter(log10_P >= 8) %>% 
      make_tally_table() %>% 
      rename(`Using p < 10-8` = n), 
    by = "Quantity") 
}

all_sig_snps %>% 
  # restrict to only papers in the original database, i.e. not those added post-publication:
  filter(Reference %in% read_csv("data/input/references_in_original_database.csv")$Reference) %>% 
  make_full_tally_table() %>% 
  kable() %>% 
  kable_styling(full_width = FALSE)
```

#### Version of the table that is correct with the current, post-publication version of the dataset

The figures in this table will be updated when new DGRP data is added to the database.

```{r results='show'}
all_sig_snps %>% 
  # do *not* restrict to only papers in the original database, i.e. not those added post-publication:
  make_full_tally_table() %>% 
  kable() %>% 
  kable_styling(full_width = FALSE)

```

<!-- ## Create a file of all the significant GWAS hits for all traits -->

<!-- ```{r compile_significant_snps} -->
<!-- all_files <- list.files("gwas_data/derived/gwas_results", full.names = T, pattern = "_significant_SNPs") -->
<!-- traits <- str_remove_all(str_remove_all(all_files, "gwas_data/derived/gwas_results/"), "_significant_SNPs[.]tsv[.]gz") -->


<!-- gwas_hits <- lapply(1:length(all_files), function(i) { -->
<!--   print(i) -->
<!--   read_tsv(all_files[i]) %>% -->
<!--     filter(P < 10e-5) %>% -->
<!--     arrange(P) %>% -->
<!--     rename(Variant = SNP, Beta = BETA) %>% -->
<!--     mutate(Trait = traits[i], .before = 1) -->
<!-- }) %>% -->
<!--   bind_rows() -->

<!-- db <- DBI::dbConnect(RSQLite::SQLite(), -->
<!--                      "~/Rprojects/fitnessGWAS/data/derived/annotations.sqlite3") -->

<!-- annotations <- tbl(db, "variants") %>%  -->
<!--   filter(SNP %in% !! unique(gwas_hits$Variant)) %>%  -->
<!--   select(-chr, -position) %>%  -->
<!--   rename(Variant = SNP) %>%  -->
<!--   collect(n=Inf) %>% distinct() -->

<!-- gene_names <- tbl(db, "genes") %>%  -->
<!--   filter(FBID %in% !! unique(annotations$FBID)) %>%  -->
<!--   select(FBID, gene_name) %>%  -->
<!--   collect(n=Inf) %>% distinct() -->

<!-- left_join(gwas_hits, annotations, by = "Variant") %>%  -->
<!--   left_join(gene_names, by = "FBID") %>%  -->
<!--   select(Trait_ID = Trait, Variant,  -->
<!--          FBID, gene_name, site_class = site.class,  -->
<!--          distance_to_gene = distance.to.gene,  -->
<!--          MAF, minor_allele, major_allele,  -->
<!--          BETA = Beta, SE, P) %>%  -->
<!--   write_tsv("gwas_data/derived/all_traits_significant_SNPs.tsv") -->

<!-- ``` -->

